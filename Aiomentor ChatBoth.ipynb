{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\os\\AppData\\Local\\Temp\\ipykernel_6120\\1659758093.py:43: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding = OpenAIEmbeddings(openai_api_key=api_key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø³Ù„Ø§Ù…! ØªØ®ØµØµ Ù…Ù† Ø¯Ø± Ø§Ø±Ø§Ø¦Ù‡ Ø±Ø§Ù‡Ú©Ø§Ø±Ù‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ùˆ Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ø±Ø§ÛŒ Ú©Ø³Ø¨ Ùˆ Ú©Ø§Ø±Ù‡Ø§ Ø¨Ø±Ø§Ø³Ø§Ø³ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ Ø§Ø³Øª. Ø¨Ø§Ø¹Ø« Ø§ÙØ²Ø§ÛŒØ´ Ø¨Ù‡Ø±Ù‡â€ŒÙˆØ±ÛŒ Ùˆ Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ù‡ØªØ± Ø¨Ø§ Ù…Ø´ØªØ±ÛŒØ§Ù† Ù…ÛŒâ€ŒØ´ÙˆÙ…. ğŸ¤–ğŸŒŸ\n",
      "Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… Ø¯Ø± Ù…ÙˆØ±Ø¯ Ú†ÛŒØ² Ø¯ÛŒÚ¯Ø±ÛŒ Ú©Ù…Ú©ØªØ§Ù† Ú©Ù†Ù…ØŸ ğŸ¤—\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import fitz  # Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø¯Ù† PDF\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ Ø§Ø² ÙØ§ÛŒÙ„ .env\n",
    "load_dotenv()\n",
    "\n",
    "# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„ÛŒØ¯ API Ø§Ø² Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒ\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø§Ø² ÙØ§ÛŒÙ„ PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        text += page.get_text(\"text\")\n",
    "    return text\n",
    "\n",
    "# ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ† Ø¨Ù‡ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©â€ŒØªØ±\n",
    "def split_text_into_chunks(text, chunk_size=1000, chunk_overlap=100):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size - chunk_overlap):\n",
    "        chunks.append(text[i:i + chunk_size])\n",
    "    return chunks\n",
    "\n",
    "# Ø°Ø®ÛŒØ±Ù‡ Ùˆ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ØªÙ‚Ø³ÛŒÙ…â€ŒØ´Ø¯Ù‡\n",
    "def save_chunks_to_json(chunks, json_filename):\n",
    "    with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(chunks, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def load_chunks_from_json(json_filename):\n",
    "    with open(json_filename, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Ø³Ø§Ø®Øª embedding Ùˆ FAISS\n",
    "def create_embedding(chunks, api_key):\n",
    "    embedding = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "    return FAISS.from_texts(chunks, embedding)\n",
    "\n",
    "# Ù¾Ø±Ø§Ù…Ù¾Øª Ø§ØµÙ„ÛŒ\n",
    "prompt_template = \"\"\"\n",
    "You a friendly assistant your name is Aiomentor(Ø¢ÛŒÙˆÙ…Ù†ØªÙˆØ±). Answer questions warmly in **Persian**. \n",
    "Keep answers short, clear, and friendly. Use emojis when appropriate. Ask for clarification if needed.\n",
    "\n",
    "Hereâ€™s the user's query:\n",
    "\"\"\"\n",
    "\n",
    "# Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ø§Ø² ÙØ§ÛŒÙ„ PDF\n",
    "def answer_question_from_pdf(pdf_path, question, api_key, json_filename=\"pdf_chunks.json\"):\n",
    "    if os.path.exists(json_filename):\n",
    "        chunks = load_chunks_from_json(json_filename)\n",
    "    else:\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        chunks = split_text_into_chunks(text)\n",
    "        save_chunks_to_json(chunks, json_filename)\n",
    "\n",
    "    vector_store = create_embedding(chunks, api_key)\n",
    "    retriever = vector_store.as_retriever()\n",
    "\n",
    "    llm = ChatOpenAI(openai_api_key=api_key, temperature=0.8)\n",
    "    chain = ConversationalRetrievalChain.from_llm(llm, retriever)\n",
    "\n",
    "    chat_history = []  # Ø§ÛŒØ¬Ø§Ø¯ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¬Ø¯ÛŒØ¯\n",
    "\n",
    "    question_with_prompt = f\"{prompt_template}{question}\"\n",
    "    \n",
    "    # ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¨Ù‡ Ø±ÙˆØ² Ù…ÛŒâ€ŒØ´ÙˆØ¯\n",
    "    response = chain.invoke({\"question\": question_with_prompt, \"chat_history\": chat_history})\n",
    "\n",
    "    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù¾Ø±Ø³Ø´ Ùˆ Ù¾Ø§Ø³Ø® Ø¨Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡\n",
    "    chat_history.append((question, response[\"answer\"]))\n",
    "\n",
    "    return response[\"answer\"]\n",
    "\n",
    "# Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø¯\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"mymentor.pdf\"\n",
    "    question = \"Ø³Ù„Ø§Ù… Ø§ÛŒÙˆÙ…Ù†ØªÙˆØ± ØªØ®ØµØµØª Ú†ÛŒÙ‡\"\n",
    "    response = answer_question_from_pdf(pdf_path, question, api_key)\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " prompt for long response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ù¾Ø±Ø§Ù…Øª Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø® Ù‡Ø§ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ ØªØ±\n",
    "prompt_template = \"\"\"\n",
    "You are a friendly and intelligent assistant named Aiomentor (Ø¢ÛŒÙˆÙ…Ù†ØªÙˆØ±). Your goal is to provide answers that are helpful, warm, and clear. \n",
    "Answer questions in **Persian**. \n",
    "While keeping answers concise, if a question requires further elaboration or explanation, provide detailed responses to ensure understanding.\n",
    "Your tone should be warm, supportive, and engaging, using emojis when appropriate to make the conversation feel friendly and approachable ğŸ˜Š. \n",
    "Make the user feel comfortable and heard. If you sense confusion or uncertainty in the question, ask for clarification in a friendly and welcoming way ğŸ¤—.\n",
    "\n",
    "Hereâ€™s the user's query:\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
